{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "iMessage-NLG-Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgdyWbTG2ija"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import *\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from itertools import zip_longest\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhEEl3gDz37l"
      },
      "source": [
        "class Counter2D(Counter):\n",
        "    def __init__(self, l: list):\n",
        "        super(Counter2D, self).__init__([x for xs in l for x in set(xs)])\n",
        "\n",
        "class iMessageVocab(object):\n",
        "    def __init__(self, text_data: List):\n",
        "        assert type(text_data) is list, \"Must pass in a list of texts\"\n",
        "        self.sents = self.__text_handler(text_data)\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.num_tokens = 3\n",
        "        self.__create_vocab()\n",
        "\n",
        "    def __text_handler(self, text_data: List): \n",
        "        #checks if text is tokenized or not, and tokenizes if not\n",
        "        if type(text_data[0]) is str and len(text_data[0].split()[0]) > 1:\n",
        "            return [x.split() for x in text_data]\n",
        "        return text_data\n",
        "\n",
        "    def __create_vocab(self):\n",
        "        self.word2index = {\n",
        "            self.pad_token: 0, \n",
        "            self.sos_token: 1,\n",
        "            self.eos_token: 2\n",
        "            }\n",
        "\n",
        "        for sentence in self.sents:\n",
        "            for word in sentence:\n",
        "                if word not in self.word2index:\n",
        "                    self.word2index[word] = self.num_tokens\n",
        "                    self.num_tokens += 1\n",
        "        self.index2word = {v:k for k,v in self.word2index.items()}\n",
        "\n",
        "    def most_common(self, n: int) -> list:\n",
        "        counts = Counter2D(self.sents)\n",
        "        return counts.most_common(n)\n",
        "\n",
        "    def __call__(self, text: Union[str, List[str]]) -> List[int]:\n",
        "        return self.transform(text)\n",
        "\n",
        "    def transform(self, text: Union[str, List[str]]) -> List[int]:\n",
        "        if type(text) is str:\n",
        "            text = text.split()\n",
        "        return [self.word2index[self.sos_token]] + [self.word2index[word] for word in text] + [self.word2index[self.eos_token]]\n",
        "\n",
        "\n",
        "class iMessage(Dataset):\n",
        "    def __init__(self, path: str):\n",
        "        self.path = path\n",
        "        self.data = self.__load()\n",
        "        self.vocab = self.create_vocab(self.data)\n",
        "        self.data = self.get_sentence_pairs(self.data) \n",
        "\n",
        "    def __load(self) -> List[str]:\n",
        "        df = pd.read_csv(self.path)\n",
        "        df.text = df.text.astype(str)\n",
        "        df.text = df.text.apply(lambda x: re.sub(r\"\\s{2,}\", \" \", x).strip())\n",
        "        df = df[df.text.str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "        df[\"msglen\"] = df.text.apply(lambda x: len(x.split()))\n",
        "        df = df[df[\"msglen\"] > 1].reset_index(drop=True)\n",
        "        df['msgkey'] = (df['is_sent'] != df['is_sent'].shift(1)).astype(int).cumsum()\n",
        "        return df.groupby('msgkey', sort=False)['text'].apply(' <message_end> '.join).tolist()\n",
        "\n",
        "    def create_vocab(self, sents):\n",
        "        return iMessageVocab(sents)\n",
        "\n",
        "    def get_sentence_pairs(self, data: List):\n",
        "        return list(zip(data[:-1], data[1:]))\n",
        "\n",
        "    def __tokenizer(self, sent: str) -> List[int]:\n",
        "        return self.vocab.transform(sent)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        x, y = self.data[idx]\n",
        "        x, y = self.__tokenizer(x), self.__tokenizer(y)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "def zero_pad(list2d: List[list], fill_with: int = 0) -> np.ndarray:\n",
        "    return np.array(list(zip_longest(*list2d, fillvalue=fill_with)))\n",
        "    #return np.array(list(zip_longest(*list2d, fillvalue=fill_with))).T\n",
        "\n",
        "\n",
        "class iMessageLoader(object):\n",
        "    def __init__(self, dataset, batch_size: int, shuffle: bool = True):\n",
        "        self.ds = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.idxs = np.arange(len(self.ds))\n",
        "        if shuffle:\n",
        "            np.random.shuffle(self.idxs)\n",
        "        self.batch_idxs = self.__get_batch_idxs()\n",
        "    \n",
        "    def __get_batch_idxs(self):\n",
        "        rem = divmod(len(self.idxs), self.batch_size)[1]\n",
        "        if rem > 0:\n",
        "            self.idxs = self.idxs[:-rem]\n",
        "        assert len(self.idxs) % self.batch_size == 0\n",
        "        split_size = int(len(self.idxs) / self.batch_size)\n",
        "        return np.array_split(self.idxs, split_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_idxs)\n",
        "\n",
        "    def var_handler(self, x, mask: bool, max_len: bool) -> Tuple:\n",
        "        lengths = torch.Tensor([len(tokens) for tokens in x])\n",
        "        if max_len:\n",
        "            lengths = max(lengths)\n",
        "        x = zero_pad(x)\n",
        "        if mask:\n",
        "            return torch.LongTensor(x), lengths, self.create_binary_mask(x)\n",
        "        return torch.LongTensor(x), lengths\n",
        "\n",
        "    def create_binary_mask(self, arr: np.ndarray) -> torch.Tensor:\n",
        "        mask_arr = np.zeros(arr.shape)\n",
        "        mask_idxs = np.argwhere(arr > 0)\n",
        "        mask_arr[mask_idxs[:,0], mask_idxs[:,1]] = 1\n",
        "        return torch.BoolTensor(mask_arr)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idxs = self.batch_idxs[idx]\n",
        "        X_batch, y_batch = [], []\n",
        "        for idx in idxs:\n",
        "            x, y = self.ds[idx]\n",
        "            X_batch.append(x)\n",
        "            y_batch.append(y)\n",
        "\n",
        "        X_batch = self.var_handler(X_batch, mask=False, max_len=False)\n",
        "        y_batch = self.var_handler(y_batch, mask=True, max_len=True)\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkwFDiII882K"
      },
      "source": [
        "PATH = \"iMessage_sample.csv\"\n",
        "batch_size = 16\n",
        "\n",
        "ds = iMessage(PATH)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGoOMy3YKb27"
      },
      "source": [
        "loader = iMessageLoader(ds, batch_size, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y61wSW98ZclD",
        "outputId": "a8dc1c2a-4b6b-42a3-e744-a98405b84211"
      },
      "source": [
        "for (X, lengths), (y, max_len, mask) in loader:\n",
        "    print(X.shape)\n",
        "    packed = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
        "    padded, _ = pad_packed_sequence(packed)\n",
        "    print(padded.shape)\n",
        "    break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([49, 16])\n",
            "torch.Size([49, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyXLTNWeR9gn",
        "outputId": "9c528e3f-04d7-4be1-b7e3-2b625660b281"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzjwLuRfo_F0"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder\n",
        "    ----------------------\n",
        "            PARAMS\n",
        "    ----------------------\n",
        "        1. 'embedding' -> embedding layer\n",
        "    ---------------------\n",
        "    Forward (__call__) Params\n",
        "    ---------------------\n",
        "        1. 'X' -> padded batch of input sequences; shape=(max_length, batch_size)\n",
        "        2. 'input_lengths' -> list of sequence lengths for each sentence in batch; shape=(batch_size)\n",
        "        3. 'hidden' -> hidden state; shape=(n_layers x n_directions, batch_size, hidden_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=True)\n",
        "\n",
        "    def forward(self, X, input_lengths, hidden=None):\n",
        "        X = self.embedding(X)\n",
        "        X = pack_padded_sequence(X, input_lengths, enforce_sorted=False) #pack to save computing power\n",
        "        outputs, hidden = self.gru(X, hidden)\n",
        "        outputs, _ = pad_packed_sequence(outputs) #outputs, lengths\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] #sum bidirectional units\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Mechanism\n",
        "    ------\n",
        "    output is softmax normalized weights tensor of shape (batch_size, 1, max_length)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        assert method in [\"dot\", \"general\", \"concat\"], f\"{method} must be either 'dot', 'general', or 'concat'\"\n",
        "        score_func_map = {\n",
        "            \"dot\": self.dot_score, \n",
        "            \"general\": self.general_score, \n",
        "            \"concat\": self.concat_score\n",
        "            }\n",
        "        self.method = method\n",
        "        self.scoring_func = score_func_map[method]\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attention = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "        elif self.method == 'concat':\n",
        "            self.attention = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        weights = self.attention(encoder_output)\n",
        "        return torch.sum(hidden * weights, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        weights = self.attention(torch.cat((hidden.expand(encoder_output.size(0), -1, -1),\n",
        "                                           encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * weights, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        attn_weights = self.scoring_func(hidden, encoder_outputs).t() #transpose\n",
        "        return F.softmax(attn_weights, dim=1).unsqueeze(1) #softmax to create probabilities and then add dim\n",
        "\n",
        "\n",
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder with Attention Mechanism\n",
        "    ----------------------\n",
        "            PARAMS\n",
        "    ----------------------\n",
        "        1. 'attention_method' -> ttention scoring method; either dot, general, or concat.\n",
        "        2. 'embedding' -> embedding layer\n",
        "    ---------------------\n",
        "    Forward (__call__) Params\n",
        "    ---------------------\n",
        "        1. 'X_step' -> One timestep of input sequence; shape=(1, batch_size)\n",
        "        2. 'prev_hidden' -> final hidden layer of GRU; shape=(n_layers x n_directions, batch_size, hidden_size)\n",
        "        3. 'encoder_output' -> encoder output; shape=(max_length, batch_size, hidden_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 attention_method, \n",
        "                 embedding, \n",
        "                 hidden_size, \n",
        "                 output_size, \n",
        "                 n_layers=1):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.attention_method = attention_method\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attention = Attention(attention_method, hidden_size)\n",
        "\n",
        "    def forward(self, X_step, prev_hidden, encoder_output):\n",
        "        #NOTE: runs one word (timestep) at a time\n",
        "        embedded = self.embedding(X_step)\n",
        "        output, hidden = self.gru(embedded, prev_hidden)\n",
        "        attention_w = self.attention(output, encoder_output) #attention weights\n",
        "        #attention weights * encoder outputs = weighted sum context vector\n",
        "        context = attention_w.bmm(encoder_output.transpose(0, 1)) #batch matrix-matrix product\n",
        "\n",
        "        concat_input = torch.cat((output.squeeze(0), context.squeeze(1)), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output) #predict next word\n",
        "        output = F.softmax(output, dim=1) \n",
        "        return output, hidden\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is36dTDcd6SA"
      },
      "source": [
        "def mask_nll_loss(decoder_output, target, mask):\n",
        "    \"\"\"\n",
        "    Average negative log likelihood of the elements that correspond to a 1 in the mask tensor\n",
        "    -----\n",
        "    PARAMS\n",
        "    -----\n",
        "\n",
        "        1. 'decoder_output' -> decoder output tensor\n",
        "        2. 'target' -> the target tensor\n",
        "        3. 'mask' -> binary mask tensor describing the padding of the target tensor\n",
        "    \"\"\"\n",
        "    n_total = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(decoder_output, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, n_total.item()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4iNvGQUOtKP"
      },
      "source": [
        "attention_method = 'dot'\n",
        "count_of_tokens = ds.vocab.num_tokens\n",
        "hidden_size = 512\n",
        "encoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "\n",
        "num_epochs = 5\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "\n",
        "\n",
        "embedding_layer = nn.Embedding(count_of_tokens, hidden_size)\n",
        "\n",
        "encoder = Encoder(hidden_size=hidden_size, \n",
        "                  embedding=embedding_layer, \n",
        "                  n_layers=2)\n",
        "\n",
        "decoder = AttentionDecoder(attention_method=attention_method, \n",
        "                           embedding=embedding_layer, \n",
        "                           hidden_size=hidden_size, \n",
        "                           output_size = count_of_tokens,\n",
        "                           n_layers=2)\n",
        "\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "#configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-jDMlTnNnWr",
        "outputId": "fa9cae17-b03c-4b16-d5b0-5f78382e435c"
      },
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "    for (X, lengths), (y, max_len, mask) in loader:\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        mask = mask.to(device)\n",
        "        lengths = lengths.to(\"cpu\") #lengths should always be on cpu\n",
        "        loss, n_totals = 0, 0\n",
        "\n",
        "        cumulative_losses = []\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(X, lengths)\n",
        "\n",
        "        decoder_input = X[0].view(1,-1).to(device)\n",
        "        #initial decoder hidden state is encoder final hidden state\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "        #if random.random() < teacher_forcing_ratio:\n",
        "        for idx in range(int(max_len)):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = y[idx].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, n_total = mask_nll_loss(decoder_output, y[idx], mask[idx])\n",
        "            loss += mask_loss\n",
        "            cumulative_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        #in place\n",
        "        nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "        nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "        # Adjust model weights\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "    print(f\"EPOCH {epoch}: {sum(cumulative_losses) / n_totals}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1: 4.756896694499657\n",
            "EPOCH 2: 4.42965586975749\n",
            "EPOCH 3: 4.161765954325365\n",
            "EPOCH 4: 3.9816492099079723\n",
            "EPOCH 5: 3.7919977152284576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgPQ5DMvovMK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}